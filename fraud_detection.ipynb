{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fraud-detection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eA-sV6ogYJz",
        "outputId": "69996202-b0f5-412f-f5f3-1a2b5ebd9128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo,encoding='bytes')\n",
        "    return dict"
      ],
      "metadata": {
        "id": "1i15-omHgad3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "the_data = unpickle('dataset.p')\n",
        "head = the_data[0,:]\n",
        "body = the_data[1:,:]\n",
        "\n",
        "# Notes: different features\n",
        "'''\n",
        "fea_history = all_fea[:,[1,12,19,35]]   # 4\n",
        "fea_rating = all_fea[:,[0,2,10,11,13,14,15,16,17,18,28,29,30,31,32]]  # 15\n",
        "fea_feedback = all_fea[:,[3,4,5,6,21,22,23,24,25,26,33,34]]  # 12\n",
        "fea_time = all_fea[:,[7,8,9,20,27]] # 5\n",
        "fea_products = all_fea[:,36:41] # 5\n",
        "fea_review = all_fea[:,41:] # 7\n",
        "'''\n",
        "\n",
        "# feature = body[:,1:]\n",
        "# label = body[:,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gWzFy3ywgd1R",
        "outputId": "fd3535cb-ef94-4a0f-d30f-46941129d0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfea_history = all_fea[:,[1,12,19,35]]   # 4\\nfea_rating = all_fea[:,[0,2,10,11,13,14,15,16,17,18,28,29,30,31,32]]  # 15\\nfea_feedback = all_fea[:,[3,4,5,6,21,22,23,24,25,26,33,34]]  # 12\\nfea_time = all_fea[:,[7,8,9,20,27]] # 5\\nfea_products = all_fea[:,36:41] # 5\\nfea_review = all_fea[:,41:] # 7\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(head)\n",
        "print(body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqildvLwgneg",
        "outputId": "6ebfe92b-7f8e-42cb-fc11-b112bbd1912f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['label' 'entro_rate' 'n_pro' 'pos_rate_ratio' 'sum_help' 'sum_unhelp'\n",
            " 'ave_help' 'ave_unhelp' 'time_gap' 'entro_time' 'act_ratio' 'mean_rate'\n",
            " 'min_rate' 'memo_len' 'mean_rate.1' 'score_1' 'score_2' 'score_3'\n",
            " 'score_4' 'score_5' 'pro_structure' 'same_date' 'max_help' 'min_help'\n",
            " 'median_help' 'max_unhelp' 'min_unhelp' 'median_unhelp' 'year_gap'\n",
            " 'percent1' 'percent2' 'percent3' 'percent4' 'percent5' 'per_help'\n",
            " 'per_unhelp' 'name_len' 'o_score' 'n_com' 'com_gap' 'com_rank'\n",
            " 'com_rank_ratio' 'user_help' 'user_unhelp' 'len_sum' 'user_com_gap'\n",
            " 'user_com_gap_ratio' 'user_rate' 'sen_rev']\n",
            "[['1' '1.16536111426297' '147' ... '0.0161354581673307' '4' '1']\n",
            " ['1' '0.578532843714968' '31' ... '0.0633928571428571' '1' '-1']\n",
            " ['0' '1.2559815759639' '110' ... '0.119694587918257' '5' '1']\n",
            " ...\n",
            " ['0' '1.33055951885479' '376' ... '0.00773993808049536' '4' '1']\n",
            " ['1' '1.13767961766797' '802' ... '0.053866203301477' '1' '0']\n",
            " ['0' '0' '2' ... '0.117802939050831' '5' '1']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrkjJPvJf9aQ",
        "outputId": "a91e1c1a-48cc-4e59-fa49-87397b589628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/rmsprop.py:188: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Epoch: 1, Test Accuracy: 0.578462\n",
            "Epoch: 2, Test Accuracy: 0.578462\n",
            "Epoch: 3, Test Accuracy: 0.578462\n",
            "Epoch: 4, Test Accuracy: 0.651795\n",
            "Epoch: 5, Test Accuracy: 0.717179\n",
            "Epoch: 6, Test Accuracy: 0.730769\n",
            "Epoch: 7, Test Accuracy: 0.741026\n",
            "Epoch: 8, Test Accuracy: 0.753077\n",
            "Epoch: 9, Test Accuracy: 0.751026\n",
            "Epoch: 10, Test Accuracy: 0.750000\n",
            "Epoch: 11, Test Accuracy: 0.754615\n",
            "Epoch: 12, Test Accuracy: 0.754615\n",
            "Epoch: 13, Test Accuracy: 0.757436\n",
            "Epoch: 14, Test Accuracy: 0.753846\n",
            "Epoch: 15, Test Accuracy: 0.760000\n",
            "Epoch: 16, Test Accuracy: 0.755385\n",
            "Epoch: 17, Test Accuracy: 0.753846\n",
            "Epoch: 18, Test Accuracy: 0.758718\n",
            "Epoch: 19, Test Accuracy: 0.762051\n",
            "Epoch: 20, Test Accuracy: 0.766923\n",
            "Epoch: 21, Test Accuracy: 0.772821\n",
            "Epoch: 22, Test Accuracy: 0.772051\n",
            "Epoch: 23, Test Accuracy: 0.770256\n",
            "Epoch: 24, Test Accuracy: 0.767949\n",
            "Epoch: 25, Test Accuracy: 0.772821\n",
            "Epoch: 26, Test Accuracy: 0.775128\n",
            "Epoch: 27, Test Accuracy: 0.774615\n",
            "Epoch: 28, Test Accuracy: 0.780513\n",
            "Epoch: 29, Test Accuracy: 0.784872\n",
            "Epoch: 30, Test Accuracy: 0.784359\n",
            "Epoch: 31, Test Accuracy: 0.789231\n",
            "Epoch: 32, Test Accuracy: 0.786154\n",
            "Epoch: 33, Test Accuracy: 0.787692\n",
            "Epoch: 34, Test Accuracy: 0.790513\n",
            "Epoch: 35, Test Accuracy: 0.791538\n",
            "Epoch: 36, Test Accuracy: 0.792821\n",
            "Epoch: 37, Test Accuracy: 0.794615\n",
            "Epoch: 38, Test Accuracy: 0.792564\n",
            "Epoch: 39, Test Accuracy: 0.798462\n",
            "Epoch: 40, Test Accuracy: 0.797436\n",
            "Epoch: 41, Test Accuracy: 0.802308\n",
            "Epoch: 42, Test Accuracy: 0.803077\n",
            "Epoch: 43, Test Accuracy: 0.806410\n",
            "Epoch: 44, Test Accuracy: 0.814359\n",
            "Epoch: 45, Test Accuracy: 0.810513\n",
            "Epoch: 46, Test Accuracy: 0.818462\n",
            "Epoch: 47, Test Accuracy: 0.823077\n",
            "Epoch: 48, Test Accuracy: 0.826923\n",
            "Epoch: 49, Test Accuracy: 0.828718\n",
            "Epoch: 50, Test Accuracy: 0.833590\n",
            "Epoch: 51, Test Accuracy: 0.833077\n",
            "Epoch: 52, Test Accuracy: 0.837949\n",
            "Epoch: 53, Test Accuracy: 0.834359\n",
            "Epoch: 54, Test Accuracy: 0.837949\n",
            "Epoch: 55, Test Accuracy: 0.841538\n",
            "Epoch: 56, Test Accuracy: 0.847179\n",
            "Epoch: 57, Test Accuracy: 0.841795\n",
            "Epoch: 58, Test Accuracy: 0.844615\n",
            "Epoch: 59, Test Accuracy: 0.847436\n",
            "Epoch: 60, Test Accuracy: 0.853846\n",
            "Epoch: 61, Test Accuracy: 0.858974\n",
            "Epoch: 62, Test Accuracy: 0.857179\n",
            "Epoch: 63, Test Accuracy: 0.848462\n",
            "Epoch: 64, Test Accuracy: 0.856667\n",
            "Epoch: 65, Test Accuracy: 0.862308\n",
            "Epoch: 66, Test Accuracy: 0.864872\n",
            "Epoch: 67, Test Accuracy: 0.865128\n",
            "Epoch: 68, Test Accuracy: 0.871282\n",
            "Epoch: 69, Test Accuracy: 0.871795\n",
            "Epoch: 70, Test Accuracy: 0.876923\n",
            "Epoch: 71, Test Accuracy: 0.881282\n",
            "Epoch: 72, Test Accuracy: 0.870256\n",
            "Epoch: 73, Test Accuracy: 0.882051\n",
            "Epoch: 74, Test Accuracy: 0.876410\n",
            "Epoch: 75, Test Accuracy: 0.880513\n",
            "Epoch: 76, Test Accuracy: 0.884359\n",
            "Epoch: 77, Test Accuracy: 0.878718\n",
            "Epoch: 78, Test Accuracy: 0.877179\n",
            "Epoch: 79, Test Accuracy: 0.893846\n",
            "Epoch: 80, Test Accuracy: 0.889231\n",
            "Epoch: 81, Test Accuracy: 0.893077\n",
            "Epoch: 82, Test Accuracy: 0.891795\n",
            "Epoch: 83, Test Accuracy: 0.897692\n",
            "Epoch: 84, Test Accuracy: 0.902564\n",
            "Epoch: 85, Test Accuracy: 0.901282\n",
            "Epoch: 86, Test Accuracy: 0.902821\n",
            "Epoch: 87, Test Accuracy: 0.902564\n",
            "Epoch: 88, Test Accuracy: 0.901538\n",
            "Epoch: 89, Test Accuracy: 0.889487\n",
            "Epoch: 90, Test Accuracy: 0.902821\n",
            "Epoch: 91, Test Accuracy: 0.901026\n",
            "Epoch: 92, Test Accuracy: 0.904872\n",
            "Epoch: 93, Test Accuracy: 0.902564\n",
            "Epoch: 94, Test Accuracy: 0.906923\n",
            "Epoch: 95, Test Accuracy: 0.916410\n",
            "Epoch: 96, Test Accuracy: 0.906923\n",
            "Epoch: 97, Test Accuracy: 0.915897\n",
            "Epoch: 98, Test Accuracy: 0.907179\n",
            "Epoch: 99, Test Accuracy: 0.906667\n",
            "Epoch: 100, Test Accuracy: 0.910513\n",
            "Epoch: 101, Test Accuracy: 0.914103\n",
            "Epoch: 102, Test Accuracy: 0.913846\n",
            "Epoch: 103, Test Accuracy: 0.903846\n",
            "Epoch: 104, Test Accuracy: 0.915641\n",
            "Epoch: 105, Test Accuracy: 0.915897\n",
            "Epoch: 106, Test Accuracy: 0.913590\n",
            "Epoch: 107, Test Accuracy: 0.912821\n",
            "Epoch: 108, Test Accuracy: 0.901026\n",
            "Epoch: 109, Test Accuracy: 0.917436\n",
            "Epoch: 110, Test Accuracy: 0.903077\n",
            "Epoch: 111, Test Accuracy: 0.908974\n",
            "Epoch: 112, Test Accuracy: 0.912821\n",
            "Epoch: 113, Test Accuracy: 0.916154\n",
            "Epoch: 114, Test Accuracy: 0.921282\n",
            "Epoch: 115, Test Accuracy: 0.892564\n",
            "Epoch: 116, Test Accuracy: 0.906410\n",
            "Epoch: 117, Test Accuracy: 0.924872\n",
            "Epoch: 118, Test Accuracy: 0.923846\n",
            "Epoch: 119, Test Accuracy: 0.921795\n",
            "Epoch: 120, Test Accuracy: 0.913077\n",
            "Epoch: 121, Test Accuracy: 0.925128\n",
            "Epoch: 122, Test Accuracy: 0.898974\n",
            "Epoch: 123, Test Accuracy: 0.913846\n",
            "Epoch: 124, Test Accuracy: 0.916154\n",
            "Epoch: 125, Test Accuracy: 0.916667\n",
            "Epoch: 126, Test Accuracy: 0.927692\n",
            "Epoch: 127, Test Accuracy: 0.919744\n",
            "Epoch: 128, Test Accuracy: 0.916923\n",
            "Epoch: 129, Test Accuracy: 0.929744\n",
            "Epoch: 130, Test Accuracy: 0.926410\n",
            "Epoch: 131, Test Accuracy: 0.926923\n",
            "Epoch: 132, Test Accuracy: 0.922308\n",
            "Epoch: 133, Test Accuracy: 0.925385\n",
            "Epoch: 134, Test Accuracy: 0.932821\n",
            "Epoch: 135, Test Accuracy: 0.919487\n",
            "Epoch: 136, Test Accuracy: 0.932308\n",
            "Epoch: 137, Test Accuracy: 0.928205\n",
            "Epoch: 138, Test Accuracy: 0.921026\n",
            "Epoch: 139, Test Accuracy: 0.935385\n",
            "Epoch: 140, Test Accuracy: 0.925385\n",
            "Epoch: 141, Test Accuracy: 0.934872\n",
            "Epoch: 142, Test Accuracy: 0.927692\n",
            "Epoch: 143, Test Accuracy: 0.934359\n",
            "Epoch: 144, Test Accuracy: 0.930256\n",
            "Epoch: 145, Test Accuracy: 0.933590\n",
            "Epoch: 146, Test Accuracy: 0.931538\n",
            "Epoch: 147, Test Accuracy: 0.935385\n",
            "Epoch: 148, Test Accuracy: 0.940513\n",
            "Epoch: 149, Test Accuracy: 0.933333\n",
            "Epoch: 150, Test Accuracy: 0.936410\n",
            "Epoch: 151, Test Accuracy: 0.941795\n",
            "Epoch: 152, Test Accuracy: 0.934872\n",
            "Epoch: 153, Test Accuracy: 0.931795\n",
            "Epoch: 154, Test Accuracy: 0.936154\n",
            "Epoch: 155, Test Accuracy: 0.933077\n",
            "Epoch: 156, Test Accuracy: 0.928462\n",
            "Epoch: 157, Test Accuracy: 0.937949\n",
            "Epoch: 158, Test Accuracy: 0.929231\n",
            "Epoch: 159, Test Accuracy: 0.935641\n",
            "Epoch: 160, Test Accuracy: 0.939231\n",
            "Epoch: 161, Test Accuracy: 0.933590\n",
            "Epoch: 162, Test Accuracy: 0.933333\n",
            "Epoch: 163, Test Accuracy: 0.946667\n",
            "Epoch: 164, Test Accuracy: 0.932051\n",
            "Epoch: 165, Test Accuracy: 0.940256\n",
            "Epoch: 166, Test Accuracy: 0.943333\n",
            "Epoch: 167, Test Accuracy: 0.943846\n",
            "Epoch: 168, Test Accuracy: 0.940256\n",
            "Epoch: 169, Test Accuracy: 0.944103\n",
            "Epoch: 170, Test Accuracy: 0.947436\n",
            "Epoch: 171, Test Accuracy: 0.946410\n",
            "Epoch: 172, Test Accuracy: 0.944615\n",
            "Epoch: 173, Test Accuracy: 0.947436\n",
            "Epoch: 174, Test Accuracy: 0.946667\n",
            "Epoch: 175, Test Accuracy: 0.939231\n",
            "Epoch: 176, Test Accuracy: 0.947949\n",
            "Epoch: 177, Test Accuracy: 0.944615\n",
            "Epoch: 178, Test Accuracy: 0.938974\n",
            "Epoch: 179, Test Accuracy: 0.943333\n",
            "Epoch: 180, Test Accuracy: 0.949231\n",
            "Epoch: 181, Test Accuracy: 0.942821\n",
            "Epoch: 182, Test Accuracy: 0.940256\n",
            "Epoch: 183, Test Accuracy: 0.934615\n",
            "Epoch: 184, Test Accuracy: 0.938974\n",
            "Epoch: 185, Test Accuracy: 0.948718\n",
            "Epoch: 186, Test Accuracy: 0.947692\n",
            "Epoch: 187, Test Accuracy: 0.953846\n",
            "Epoch: 188, Test Accuracy: 0.939744\n",
            "Epoch: 189, Test Accuracy: 0.951538\n",
            "Epoch: 190, Test Accuracy: 0.943590\n",
            "Epoch: 191, Test Accuracy: 0.941026\n",
            "Epoch: 192, Test Accuracy: 0.948205\n",
            "Epoch: 193, Test Accuracy: 0.951026\n",
            "Epoch: 194, Test Accuracy: 0.937949\n",
            "Epoch: 195, Test Accuracy: 0.949744\n",
            "Epoch: 196, Test Accuracy: 0.954615\n",
            "Epoch: 197, Test Accuracy: 0.950769\n",
            "Epoch: 198, Test Accuracy: 0.944359\n",
            "Epoch: 199, Test Accuracy: 0.953077\n",
            "Epoch: 200, Test Accuracy: 0.953077\n",
            "Epoch: 201, Test Accuracy: 0.950513\n",
            "Epoch: 202, Test Accuracy: 0.951026\n",
            "Epoch: 203, Test Accuracy: 0.926154\n",
            "Epoch: 204, Test Accuracy: 0.954872\n",
            "Epoch: 205, Test Accuracy: 0.950256\n",
            "Epoch: 206, Test Accuracy: 0.950256\n",
            "Epoch: 207, Test Accuracy: 0.951795\n",
            "Epoch: 208, Test Accuracy: 0.947692\n",
            "Epoch: 209, Test Accuracy: 0.955641\n",
            "Epoch: 210, Test Accuracy: 0.956923\n",
            "Epoch: 211, Test Accuracy: 0.955641\n",
            "Epoch: 212, Test Accuracy: 0.955128\n",
            "Epoch: 213, Test Accuracy: 0.950000\n",
            "Epoch: 214, Test Accuracy: 0.954359\n",
            "Epoch: 215, Test Accuracy: 0.949744\n",
            "Epoch: 216, Test Accuracy: 0.953590\n",
            "Epoch: 217, Test Accuracy: 0.956410\n",
            "Epoch: 218, Test Accuracy: 0.957179\n",
            "Epoch: 219, Test Accuracy: 0.954103\n",
            "Epoch: 220, Test Accuracy: 0.953846\n",
            "Epoch: 221, Test Accuracy: 0.955641\n",
            "Epoch: 222, Test Accuracy: 0.951795\n",
            "Epoch: 223, Test Accuracy: 0.952564\n",
            "Epoch: 224, Test Accuracy: 0.958205\n",
            "Epoch: 225, Test Accuracy: 0.947949\n",
            "Epoch: 226, Test Accuracy: 0.950256\n",
            "Epoch: 227, Test Accuracy: 0.957692\n",
            "Epoch: 228, Test Accuracy: 0.955897\n",
            "Epoch: 229, Test Accuracy: 0.954359\n",
            "Epoch: 230, Test Accuracy: 0.957692\n",
            "Epoch: 231, Test Accuracy: 0.954615\n",
            "Epoch: 232, Test Accuracy: 0.955897\n",
            "Epoch: 233, Test Accuracy: 0.952564\n",
            "Epoch: 234, Test Accuracy: 0.958205\n",
            "Epoch: 235, Test Accuracy: 0.952051\n",
            "Epoch: 236, Test Accuracy: 0.955641\n",
            "Epoch: 237, Test Accuracy: 0.956667\n",
            "Epoch: 238, Test Accuracy: 0.954359\n",
            "Epoch: 239, Test Accuracy: 0.959744\n",
            "Epoch: 240, Test Accuracy: 0.948462\n",
            "Epoch: 241, Test Accuracy: 0.959744\n",
            "Epoch: 242, Test Accuracy: 0.960256\n",
            "Epoch: 243, Test Accuracy: 0.963333\n",
            "Epoch: 244, Test Accuracy: 0.962564\n",
            "Epoch: 245, Test Accuracy: 0.952821\n",
            "Epoch: 246, Test Accuracy: 0.957949\n",
            "Epoch: 247, Test Accuracy: 0.961026\n",
            "Epoch: 248, Test Accuracy: 0.953077\n",
            "Epoch: 249, Test Accuracy: 0.962051\n",
            "Epoch: 250, Test Accuracy: 0.964103\n",
            "Epoch: 251, Test Accuracy: 0.957949\n",
            "Epoch: 252, Test Accuracy: 0.960513\n",
            "Epoch: 253, Test Accuracy: 0.963077\n",
            "Epoch: 254, Test Accuracy: 0.956154\n",
            "Epoch: 255, Test Accuracy: 0.965385\n",
            "Epoch: 256, Test Accuracy: 0.960513\n",
            "Epoch: 257, Test Accuracy: 0.964615\n",
            "Epoch: 258, Test Accuracy: 0.961026\n",
            "Epoch: 259, Test Accuracy: 0.951538\n",
            "Epoch: 260, Test Accuracy: 0.960513\n",
            "Epoch: 261, Test Accuracy: 0.957692\n",
            "Epoch: 262, Test Accuracy: 0.960256\n",
            "Epoch: 263, Test Accuracy: 0.949231\n",
            "Epoch: 264, Test Accuracy: 0.962564\n",
            "Epoch: 265, Test Accuracy: 0.961795\n",
            "Epoch: 266, Test Accuracy: 0.964872\n",
            "Epoch: 267, Test Accuracy: 0.958974\n",
            "Epoch: 268, Test Accuracy: 0.960769\n",
            "Epoch: 269, Test Accuracy: 0.957436\n",
            "Epoch: 270, Test Accuracy: 0.961795\n",
            "Epoch: 271, Test Accuracy: 0.964615\n",
            "Epoch: 272, Test Accuracy: 0.962564\n",
            "Epoch: 273, Test Accuracy: 0.964103\n",
            "Epoch: 274, Test Accuracy: 0.955897\n",
            "Epoch: 275, Test Accuracy: 0.958974\n",
            "Epoch: 276, Test Accuracy: 0.959231\n",
            "Epoch: 277, Test Accuracy: 0.966667\n",
            "Epoch: 278, Test Accuracy: 0.961538\n",
            "Epoch: 279, Test Accuracy: 0.963333\n",
            "Epoch: 280, Test Accuracy: 0.961282\n",
            "Epoch: 281, Test Accuracy: 0.961795\n",
            "Epoch: 282, Test Accuracy: 0.964872\n",
            "Epoch: 283, Test Accuracy: 0.962308\n",
            "Epoch: 284, Test Accuracy: 0.963846\n",
            "Epoch: 285, Test Accuracy: 0.964872\n",
            "Epoch: 286, Test Accuracy: 0.958974\n",
            "Epoch: 287, Test Accuracy: 0.958718\n",
            "Epoch: 288, Test Accuracy: 0.964359\n",
            "Epoch: 289, Test Accuracy: 0.964615\n",
            "Epoch: 290, Test Accuracy: 0.967692\n",
            "Epoch: 291, Test Accuracy: 0.965641\n",
            "Epoch: 292, Test Accuracy: 0.962821\n",
            "Epoch: 293, Test Accuracy: 0.955897\n",
            "Epoch: 294, Test Accuracy: 0.961026\n",
            "Epoch: 295, Test Accuracy: 0.965385\n",
            "Epoch: 296, Test Accuracy: 0.960513\n",
            "Epoch: 297, Test Accuracy: 0.962564\n",
            "Epoch: 298, Test Accuracy: 0.962821\n",
            "Epoch: 299, Test Accuracy: 0.960513\n",
            "Epoch: 300, Test Accuracy: 0.967436\n",
            "Epoch: 301, Test Accuracy: 0.966923\n",
            "Epoch: 302, Test Accuracy: 0.967949\n",
            "Epoch: 303, Test Accuracy: 0.963077\n",
            "Epoch: 304, Test Accuracy: 0.966667\n",
            "Epoch: 305, Test Accuracy: 0.957949\n",
            "Epoch: 306, Test Accuracy: 0.967179\n",
            "Epoch: 307, Test Accuracy: 0.963077\n",
            "Epoch: 308, Test Accuracy: 0.954872\n",
            "Epoch: 309, Test Accuracy: 0.957949\n",
            "Epoch: 310, Test Accuracy: 0.965385\n",
            "Epoch: 311, Test Accuracy: 0.965897\n",
            "Epoch: 312, Test Accuracy: 0.967949\n",
            "Epoch: 313, Test Accuracy: 0.965128\n",
            "Epoch: 314, Test Accuracy: 0.966923\n",
            "Epoch: 315, Test Accuracy: 0.965128\n",
            "Epoch: 316, Test Accuracy: 0.965641\n",
            "Epoch: 317, Test Accuracy: 0.963333\n",
            "Epoch: 318, Test Accuracy: 0.964615\n",
            "Epoch: 319, Test Accuracy: 0.968462\n",
            "Epoch: 320, Test Accuracy: 0.965641\n",
            "Epoch: 321, Test Accuracy: 0.965128\n",
            "Epoch: 322, Test Accuracy: 0.965385\n",
            "Epoch: 323, Test Accuracy: 0.964103\n",
            "Epoch: 324, Test Accuracy: 0.963333\n",
            "Epoch: 325, Test Accuracy: 0.965641\n",
            "Epoch: 326, Test Accuracy: 0.965897\n",
            "Epoch: 327, Test Accuracy: 0.966923\n",
            "Epoch: 328, Test Accuracy: 0.965641\n",
            "Epoch: 329, Test Accuracy: 0.967949\n",
            "Epoch: 330, Test Accuracy: 0.966923\n",
            "Epoch: 331, Test Accuracy: 0.965641\n",
            "Epoch: 332, Test Accuracy: 0.968974\n",
            "Epoch: 333, Test Accuracy: 0.968205\n",
            "Epoch: 334, Test Accuracy: 0.965385\n",
            "Epoch: 335, Test Accuracy: 0.965385\n",
            "Epoch: 336, Test Accuracy: 0.967692\n",
            "Epoch: 337, Test Accuracy: 0.963590\n",
            "Epoch: 338, Test Accuracy: 0.964872\n",
            "Epoch: 339, Test Accuracy: 0.968974\n",
            "Epoch: 340, Test Accuracy: 0.964103\n",
            "Epoch: 341, Test Accuracy: 0.964615\n",
            "Epoch: 342, Test Accuracy: 0.962821\n",
            "Epoch: 343, Test Accuracy: 0.966410\n",
            "Epoch: 344, Test Accuracy: 0.967692\n",
            "Epoch: 345, Test Accuracy: 0.970000\n",
            "Epoch: 346, Test Accuracy: 0.964103\n",
            "Epoch: 347, Test Accuracy: 0.968205\n",
            "Epoch: 348, Test Accuracy: 0.964359\n",
            "Epoch: 349, Test Accuracy: 0.966923\n",
            "Epoch: 350, Test Accuracy: 0.945385\n",
            "Epoch: 351, Test Accuracy: 0.966410\n",
            "Epoch: 352, Test Accuracy: 0.966154\n",
            "Epoch: 353, Test Accuracy: 0.968462\n",
            "Epoch: 354, Test Accuracy: 0.967436\n",
            "Epoch: 355, Test Accuracy: 0.971282\n",
            "Epoch: 356, Test Accuracy: 0.968462\n",
            "Epoch: 357, Test Accuracy: 0.968718\n",
            "Epoch: 358, Test Accuracy: 0.968718\n",
            "Epoch: 359, Test Accuracy: 0.971282\n",
            "Epoch: 360, Test Accuracy: 0.966923\n",
            "Epoch: 361, Test Accuracy: 0.969744\n",
            "Epoch: 362, Test Accuracy: 0.964872\n",
            "Epoch: 363, Test Accuracy: 0.970769\n",
            "Epoch: 364, Test Accuracy: 0.970256\n",
            "Epoch: 365, Test Accuracy: 0.964103\n",
            "Epoch: 366, Test Accuracy: 0.967949\n",
            "Epoch: 367, Test Accuracy: 0.969487\n",
            "Epoch: 368, Test Accuracy: 0.966410\n",
            "Epoch: 369, Test Accuracy: 0.966154\n",
            "Epoch: 370, Test Accuracy: 0.954615\n",
            "Epoch: 371, Test Accuracy: 0.967436\n",
            "Epoch: 372, Test Accuracy: 0.970000\n",
            "Epoch: 373, Test Accuracy: 0.967179\n",
            "Epoch: 374, Test Accuracy: 0.967436\n",
            "Epoch: 375, Test Accuracy: 0.969231\n",
            "Epoch: 376, Test Accuracy: 0.968718\n",
            "Epoch: 377, Test Accuracy: 0.970769\n",
            "Epoch: 378, Test Accuracy: 0.970256\n",
            "Epoch: 379, Test Accuracy: 0.967179\n",
            "Epoch: 380, Test Accuracy: 0.970513\n",
            "Epoch: 381, Test Accuracy: 0.965128\n",
            "Epoch: 382, Test Accuracy: 0.966667\n",
            "Epoch: 383, Test Accuracy: 0.971282\n",
            "Epoch: 384, Test Accuracy: 0.967949\n",
            "Epoch: 385, Test Accuracy: 0.966923\n",
            "Epoch: 386, Test Accuracy: 0.970769\n",
            "Epoch: 387, Test Accuracy: 0.967436\n",
            "Epoch: 388, Test Accuracy: 0.970513\n",
            "Epoch: 389, Test Accuracy: 0.970769\n",
            "Epoch: 390, Test Accuracy: 0.971282\n",
            "Epoch: 391, Test Accuracy: 0.971795\n",
            "Epoch: 392, Test Accuracy: 0.969744\n",
            "Epoch: 393, Test Accuracy: 0.966154\n",
            "Epoch: 394, Test Accuracy: 0.969487\n",
            "Epoch: 395, Test Accuracy: 0.969231\n",
            "Epoch: 396, Test Accuracy: 0.971026\n",
            "Epoch: 397, Test Accuracy: 0.968462\n",
            "Epoch: 398, Test Accuracy: 0.964872\n",
            "Epoch: 399, Test Accuracy: 0.964615\n",
            "Epoch: 400, Test Accuracy: 0.969487\n"
          ]
        }
      ],
      "source": [
        "def reconstructlabel(label):\n",
        "    length = len(label)\n",
        "    newlabel = []\n",
        "    for index in range(length):\n",
        "        if label[index] == 0:\n",
        "            newlabel.append([0, 1])\n",
        "        else:\n",
        "            newlabel.append([1, 0])\n",
        "    return newlabel\n",
        "\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "\n",
        "# constructing neural random forest\n",
        "def model(input_layer, w_t_e, w_d_e, w_l_e, p_keep_hidden):\n",
        "    assert (len(w_t_e) == len(w_d_e))\n",
        "    assert (len(w_t_e) == len(w_l_e))\n",
        "    decision_p_e = []\n",
        "    leaf_p_e = []\n",
        "    for w_t, w_d, w_l in zip(w_t_e, w_d_e, w_l_e):\n",
        "        tree_layer = tf.nn.relu(tf.matmul(input_layer, w_t))\n",
        "        tree_layer = tf.nn.dropout(tree_layer, p_keep_hidden)\n",
        "\n",
        "        decision_p = tf.nn.sigmoid(tf.matmul(tree_layer, w_d)) \n",
        "        leaf_p = tf.nn.softmax(w_l) \n",
        "\n",
        "        decision_p_e.append(decision_p)\n",
        "        leaf_p_e.append(leaf_p)\n",
        "    return decision_p_e, leaf_p_e\n",
        "\n",
        "\n",
        "def neural_random_forest(decision_p_e, leaf_p_e, n_depth, n_leaf, n_batch):\n",
        "    flat_decision_p_e = []\n",
        "\n",
        "    for decision_p in decision_p_e:\n",
        "        decision_p_comp = tf.subtract(tf.ones_like(decision_p), decision_p)\n",
        "        decision_p_pack = tf.stack([decision_p, decision_p_comp])\n",
        "        flat_decision_p = tf.reshape(decision_p_pack, [-1])\n",
        "        flat_decision_p_e.append(flat_decision_p)\n",
        "\n",
        "    batch_0_indices = \\\n",
        "        tf.tile(tf.expand_dims(tf.range(0, n_batch * n_leaf, n_leaf), 1), [1, n_leaf])\n",
        "\n",
        "    # The routing probability computation\n",
        "    in_repeat = int(n_leaf / 2)\n",
        "    out_repeat = n_batch\n",
        "\n",
        "    batch_complement_indices = \\\n",
        "        np.array([[0] * in_repeat, [n_batch * n_leaf] * in_repeat]\n",
        "                 * out_repeat).reshape(n_batch, n_leaf)\n",
        "\n",
        "    mu_e = []\n",
        "    # iterate over each tree\n",
        "    for i, flat_decision_p in enumerate(flat_decision_p_e):\n",
        "        mu = tf.gather(flat_decision_p, tf.add(batch_0_indices, batch_complement_indices))\n",
        "        mu_e.append(mu)\n",
        "\n",
        "    # from the second layer to the last layer, we make the decision nodes\n",
        "    for d in range(1, n_depth + 1):\n",
        "        indices = tf.range(2 ** d, 2 ** (d + 1)) - 1\n",
        "        tile_indices = tf.reshape(tf.tile(tf.expand_dims(indices, 1),\n",
        "                                          [1, 2 ** (n_depth - d + 1)]), [1, -1])\n",
        "        batch_indices = tf.add(batch_0_indices, tf.tile(tile_indices, [n_batch, 1]))\n",
        "\n",
        "        in_repeat = int(in_repeat / 2)\n",
        "        out_repeat = int(out_repeat * 2)\n",
        "\n",
        "        batch_complement_indices = \\\n",
        "            np.array([[0] * in_repeat, [n_batch * n_leaf] * in_repeat]\n",
        "                     * out_repeat).reshape(n_batch, n_leaf)\n",
        "\n",
        "        mu_e_update = []\n",
        "        for mu, flat_decision_p in zip(mu_e, flat_decision_p_e):\n",
        "            mu = tf.multiply(mu, tf.gather(flat_decision_p, tf.add(batch_indices, batch_complement_indices)))\n",
        "            mu_e_update.append(mu)\n",
        "\n",
        "        mu_e = mu_e_update\n",
        "\n",
        "    return mu_e\n",
        "\n",
        "\n",
        "def probability_y_x(mu_e, leaf_p_e):\n",
        "    py_x_e = []\n",
        "    py_x_leaf_e = []\n",
        "    for mu, leaf_p in zip(mu_e, leaf_p_e):\n",
        "        # average all the leaf p\n",
        "        py_x_tree = tf.reduce_sum(\n",
        "            tf.multiply(tf.tile(tf.expand_dims(mu, 2), [1, 1, N_LABEL]),\n",
        "                        tf.tile(tf.expand_dims(leaf_p, 0), [N_BATCH, 1, 1])), 1)\n",
        "        a_tree = tf.multiply(tf.tile(tf.expand_dims(mu, 2), [1, 1, N_LABEL]),\n",
        "                             tf.tile(tf.expand_dims(leaf_p, 0), [N_BATCH, 1, 1]))\n",
        "        py_x_e.append(py_x_tree)\n",
        "        py_x_leaf_e.append(a_tree)\n",
        "\n",
        "    py_x_e = tf.stack(py_x_e)\n",
        "    py_x = tf.reduce_sum(py_x_e, 0)\n",
        "    final = tf.nn.softmax(py_x)\n",
        "    # py_x_leaf_e = tf.stack(py_x_leaf_e)\n",
        "    return final\n",
        "\n",
        "\n",
        "def init_prob_weights(shape, minval=-5, maxval=5):\n",
        "    return tf.Variable(tf.random_uniform(shape, minval, maxval))\n",
        "\n",
        "\n",
        "# ======================loading================================== #\n",
        "the_data = unpickle('dataset.p')\n",
        "head = the_data[0,:]\n",
        "body = the_data[1:,:]\n",
        "\n",
        "# Notes: different features\n",
        "'''\n",
        "fea_history = all_fea[:,[1,12,19,35]]   # 4\n",
        "fea_rating = all_fea[:,[0,2,10,11,13,14,15,16,17,18,28,29,30,31,32]]  # 15\n",
        "fea_feedback = all_fea[:,[3,4,5,6,21,22,23,24,25,26,33,34]]  # 12\n",
        "fea_time = all_fea[:,[7,8,9,20,27]] # 5\n",
        "fea_products = all_fea[:,36:41] # 5\n",
        "fea_review = all_fea[:,41:] # 7\n",
        "'''\n",
        "\n",
        "feature = body[:,1:]\n",
        "label = body[:,0]\n",
        "\n",
        "N_feature = 48\n",
        "\n",
        "# Scalling\n",
        "# feature_normalized = feature\n",
        "feature_normalized=preprocessing.scale(feature)\n",
        "# feature_normalized=preprocessing.minmax_scale(feature,feature_range=(0,1))\n",
        "# feature_normalized=feature/sum(feature)\n",
        "\n",
        "# Set training and testing\n",
        "trX = feature_normalized[0:4000]\n",
        "teX = feature_normalized[4000:]\n",
        "\n",
        "newlabel=reconstructlabel(label.astype(int))\n",
        "trY = np.array(newlabel[0:4000])\n",
        "teY = np.array(newlabel[4000:])\n",
        "#\n",
        "trX = trX.reshape(-1, N_feature)\n",
        "teX = teX.reshape(-1, N_feature)\n",
        "\n",
        "# Parameter Settings\n",
        "DEPTH   = 3                 # Depth of a tree\n",
        "N_LEAF  = 2 ** (DEPTH + 1)  # Number of leaf node \n",
        "N_LABEL = 2                # Number of classes\n",
        "N_TREE  = 5                 # Number of trees (ensemble)\n",
        "N_BATCH = 50             # Number of data points per mini-batch\n",
        "\n",
        "# Input X, output Y\n",
        "x = tf.placeholder(\"float\", [N_BATCH, N_feature])\n",
        "y = tf.placeholder(\"float\", [N_BATCH, N_LABEL])\n",
        "\n",
        "p_keep_conv = tf.placeholder(\"float\")\n",
        "p_keep_hidden = tf.placeholder(\"float\")\n",
        "\n",
        "# ====================Autoencoder========================================#\n",
        "num_hidden_1 = 64  \n",
        "num_hidden_2 = 96  \n",
        "# num_hidden_3 = 64\n",
        "# num_hidden_4 = 96\n",
        "num_input = N_feature  \n",
        "\n",
        "weights = {\n",
        "    'encoder_h1': tf.Variable(tf.truncated_normal([num_input, num_hidden_1], stddev=0.1)),\n",
        "    'encoder_h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_hidden_2], stddev=0.1)),\n",
        "    # 'encoder_h3': tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_3], stddev=0.1)),\n",
        "    # 'encoder_h4': tf.Variable(tf.truncated_normal([num_hidden_3, num_hidden_4], stddev=0.1)),\n",
        "    # 'decoder_h1': tf.Variable(tf.truncated_normal([num_hidden_4, num_hidden_3], stddev=0.1)),\n",
        "    # 'decoder_h2': tf.Variable(tf.truncated_normal([num_hidden_3, num_hidden_2], stddev=0.1)),\n",
        "    'decoder_h1': tf.Variable(tf.truncated_normal([num_hidden_2, num_hidden_1], stddev=0.1)),\n",
        "    'decoder_h2': tf.Variable(tf.truncated_normal([num_hidden_1, num_input], stddev=0.1))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'encoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_1])),\n",
        "    'encoder_b2': tf.Variable(tf.constant(0.1, shape=[num_hidden_2])),\n",
        "    # 'encoder_b3': tf.Variable(tf.constant(0.1, shape=[num_hidden_3])),\n",
        "    # 'encoder_b4': tf.Variable(tf.constant(0.1, shape=[num_hidden_4])),\n",
        "    # 'decoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_3])),\n",
        "    # 'decoder_b2': tf.Variable(tf.constant(0.1, shape=[num_hidden_2])),\n",
        "    'decoder_b1': tf.Variable(tf.constant(0.1, shape=[num_hidden_1])),\n",
        "    'decoder_b2': tf.Variable(tf.constant(0.1, shape=[num_input]))\n",
        "}\n",
        "\n",
        "\n",
        "# Building the encoder\n",
        "def encoder(x):\n",
        "    # Encoder Hidden layer with sigmoid activation #1\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
        "                                   biases['encoder_b1']))\n",
        "    # Encoder Hidden layer with sigmoid activation #2\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
        "                                   biases['encoder_b2']))\n",
        "    # layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['encoder_h3']),\n",
        "    #                                biases['encoder_b3']))\n",
        "    # layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['encoder_h4']),\n",
        "    #                                biases['encoder_b4']))\n",
        "\n",
        "    return layer_2\n",
        "\n",
        "\n",
        "# Building the decoder\n",
        "def decoder(x):\n",
        "    # Decoder Hidden layer with sigmoid activation #1\n",
        "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
        "                                   biases['decoder_b1']))\n",
        "    # Decoder Hidden layer with sigmoid activation #2\n",
        "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
        "                                   biases['decoder_b2']))\n",
        "\n",
        "    # layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
        "    #                                biases['decoder_b3']))\n",
        "    # layer_4 = tf.nn.sigmoid(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n",
        "    #                                biases['decoder_b4']))\n",
        "    return layer_2\n",
        "\n",
        "# Construct model\n",
        "encoder_op = encoder(x)\n",
        "decoder_op = decoder(encoder_op)\n",
        "\n",
        "# Prediction\n",
        "y_pred = decoder_op\n",
        "# Targets (Labels) are the input data.\n",
        "y_true = x\n",
        "\n",
        "# ====================Neural decision forest=================================#\n",
        "num_out = 96\n",
        "num_output = 128\n",
        "N_nodes = 256\n",
        "\n",
        "W_hidden = weight_variable([num_out, num_output])\n",
        "b_hidden = weight_variable([num_output])\n",
        "tree_input = tf.nn.relu(tf.matmul(encoder_op, W_hidden) + b_hidden)\n",
        "tree_input = tf.nn.dropout(tree_input, p_keep_conv)\n",
        "\n",
        "w_t_ensemble = []\n",
        "w_d_ensemble = []\n",
        "w_l_ensemble = []\n",
        "for i in range(N_TREE):\n",
        "    w_t_ensemble.append(weight_variable([num_output, N_nodes])) \n",
        "    w_d_ensemble.append(init_prob_weights([N_nodes, N_LEAF], -1, 1)) \n",
        "    w_l_ensemble.append(\n",
        "        init_prob_weights([N_LEAF, N_LABEL], -2, 2))  \n",
        "\n",
        "decision_p_e, leaf_p_e = model(tree_input, w_t_ensemble, w_d_ensemble, w_l_ensemble, p_keep_hidden)\n",
        "mu_e = neural_random_forest(decision_p_e, leaf_p_e, DEPTH, N_LEAF, N_BATCH)\n",
        "py_x = probability_y_x(mu_e, leaf_p_e) \n",
        "\n",
        "# loss function\n",
        "cost = tf.add(tf.reduce_mean(-tf.multiply(tf.log(py_x), y)),tf.reduce_mean(tf.pow(y_true - y_pred, 2)))\n",
        "train_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
        "predict = tf.argmax(py_x, 1)\n",
        "\n",
        "# =============== training ========================== #\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(400):\n",
        "    # One epoch\n",
        "    for start, end in zip(range(0, len(trX), N_BATCH), range(N_BATCH, len(trX), N_BATCH)):\n",
        "        sess.run(train_step, feed_dict={x: trX[start:end], y: trY[start:end],\n",
        "                                        p_keep_conv: 0.8, p_keep_hidden: 0.5})\n",
        "    results = []\n",
        "    for start, end in zip(range(0, len(teX), N_BATCH), range(N_BATCH, len(teX), N_BATCH)):\n",
        "        results.extend(np.argmax(teY[start:end], axis=1) ==\n",
        "                        sess.run(predict, feed_dict={x: teX[start:end], p_keep_conv:1.0, p_keep_hidden: 1.0}))\n",
        "    print('Epoch: %d, Test Accuracy: %f' % (i + 1, np.mean(results)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W99wsmuvgU_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}